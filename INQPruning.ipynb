{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################Build Essential DenseNet to Load Pretrained Parameters############################################### \n",
    "# encoding=utf8  \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def unpickle(file):\n",
    "  import _pickle as cPickle\n",
    "  fo = open(file, 'rb')\n",
    "  dict = cPickle.load(fo,encoding='latin1')\n",
    "  fo.close()\n",
    "  if 'data' in dict:\n",
    "    dict['data'] = dict['data'].reshape((-1, 3, 32, 32)).swapaxes(1, 3).swapaxes(1, 2).reshape(-1, 32*32*3) / 256.\n",
    "\n",
    "  return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_one(f):\n",
    "  batch = unpickle(f)\n",
    "  data = batch['data']\n",
    "  labels = batch['labels']\n",
    "  print (\"Loading %s: %d\" % (f, len(data)))\n",
    "  return data, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_data(files, data_dir, label_count):\n",
    "  data, labels = load_data_one(data_dir + '/' + files[0])\n",
    "  for f in files[1:]:\n",
    "    data_n, labels_n = load_data_one(data_dir + '/' + f)\n",
    "    data = np.append(data, data_n, axis=0)\n",
    "    labels = np.append(labels, labels_n, axis=0)\n",
    "  labels = np.array([ [ float(i == label) for i in range(label_count) ] for label in labels ])\n",
    "  return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_in_batch_avg(session, tensors, batch_placeholders, feed_dict={}, batch_size=200):                              \n",
    "  res = [ 0 ] * len(tensors)                                                                                           \n",
    "  batch_tensors = [ (placeholder, feed_dict[ placeholder ]) for placeholder in batch_placeholders ]                    \n",
    "  total_size = len(batch_tensors[0][1])                                                                                \n",
    "  batch_count = (total_size + batch_size - 1) / batch_size                                                             \n",
    "  for batch_idx in range(batch_count):                                                                                \n",
    "    current_batch_size = None                                                                                          \n",
    "    for (placeholder, tensor) in batch_tensors:                                                                        \n",
    "      batch_tensor = tensor[ batch_idx*batch_size : (batch_idx+1)*batch_size ]                                         \n",
    "      current_batch_size = len(batch_tensor)                                                                           \n",
    "      feed_dict[placeholder] = tensor[ batch_idx*batch_size : (batch_idx+1)*batch_size ]                               \n",
    "    tmp = session.run(tensors, feed_dict=feed_dict)                                                                    \n",
    "    res = [ r + t * current_batch_size for (r, t) in zip(res, tmp) ]                                                   \n",
    "  return [ r / float(total_size) for r in res ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.01, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def conv2d(input, in_features, out_features, kernel_size, with_bias=False):\n",
    "  W = weight_variable([ kernel_size, kernel_size, in_features, out_features ])\n",
    "  conv = tf.nn.conv2d(input, W, [ 1, 1, 1, 1 ], padding='SAME')\n",
    "  if with_bias:\n",
    "    return conv + bias_variable([ out_features ])\n",
    "  return conv\n",
    "\n",
    "def batch_activ_conv(current, in_features, out_features, kernel_size, is_training, keep_prob):\n",
    "  current = tf.contrib.layers.batch_norm(current, scale=True, is_training=is_training, updates_collections=None)\n",
    "  current = tf.nn.relu(current)\n",
    "  current = conv2d(current, in_features, out_features, kernel_size)\n",
    "  current = tf.nn.dropout(current, keep_prob)\n",
    "  return current\n",
    "\n",
    "def block(input, layers, in_features, growth, is_training, keep_prob):\n",
    "  current = input\n",
    "  features = in_features\n",
    "  for idx in range(layers):\n",
    "    tmp = batch_activ_conv(current, features, growth, 3, is_training, keep_prob)\n",
    "    current = tf.concat((current, tmp),3)\n",
    "    features += growth\n",
    "  return current, features\n",
    "\n",
    "def avg_pool(input, s):\n",
    "  return tf.nn.avg_pool(input, [ 1, s, s, 1 ], [1, s, s, 1 ], 'VALID')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = './data'\n",
    "image_size = 32\n",
    "image_dim = image_size * image_size * 3\n",
    "# meta = unpickle(data_dir + '/batches.meta')\n",
    "# label_names = meta['label_names']\n",
    "# label_count = len(label_names)\n",
    "label_count = 10\n",
    "# train_files = [ 'data_batch_%d' % d for d in range(1, 6) ]\n",
    "# train_data, train_labels = load_data(train_files, data_dir, label_count)\n",
    "# pi = np.random.permutation(len(train_data))\n",
    "# train_data, train_labels = train_data[pi], train_labels[pi]\n",
    "# test_data, test_labels = load_data([ 'test_batch' ], data_dir, label_count)\n",
    "# print (\"Train:\", np.shape(train_data), np.shape(train_labels))\n",
    "# print (\"Test:\", np.shape(test_data), np.shape(test_labels))\n",
    "# data = { 'train_data': train_data,\n",
    "#   'train_labels': train_labels,\n",
    "#   'test_data': test_data,\n",
    "#   'test_labels': test_labels }\n",
    "depth = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "weight_decay = 1e-4\n",
    "layers = int((depth - 4) / 3)\n",
    "graph = tf.Graph()\n",
    "\n",
    "xs = tf.placeholder(\"float\", shape=[None, image_dim])\n",
    "ys = tf.placeholder(\"float\", shape=[None, label_count])\n",
    "lr = tf.placeholder(\"float\", shape=[])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "is_training = tf.placeholder(\"bool\", shape=[])\n",
    "\n",
    "\n",
    "current = tf.reshape(xs, [ -1, 32, 32, 3 ])\n",
    "current = conv2d(current, 3, 16, 3)\n",
    "\n",
    "current, features = block(current, layers, 16, 12, is_training, keep_prob)\n",
    "current = batch_activ_conv(current, features, features, 1, is_training, keep_prob)\n",
    "current = avg_pool(current, 2)\n",
    "current, features = block(current, layers, features, 12, is_training, keep_prob)\n",
    "current = batch_activ_conv(current, features, features, 1, is_training, keep_prob)\n",
    "current = avg_pool(current, 2)\n",
    "current, features = block(current, layers, features, 12, is_training, keep_prob)\n",
    "\n",
    "current = tf.contrib.layers.batch_norm(current, scale=True, is_training=is_training, updates_collections=None)\n",
    "current = tf.nn.relu(current)\n",
    "current = avg_pool(current, 8)\n",
    "final_dim = features\n",
    "current = tf.reshape(current, [ -1, final_dim ])\n",
    "Wfc = weight_variable([ final_dim, label_count ])\n",
    "bfc = bias_variable([ label_count ])\n",
    "ys_ = tf.nn.softmax( tf.matmul(current, Wfc) + bfc )\n",
    "\n",
    "cross_entropy = -tf.reduce_mean(ys * tf.log(ys_ + 1e-12))\n",
    "l2 = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])\n",
    "train_step = tf.train.MomentumOptimizer(lr, 0.9, use_nesterov=True).minimize(cross_entropy + l2 * weight_decay)\n",
    "correct_prediction = tf.equal(tf.argmax(ys_, 1), tf.argmax(ys, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_dict={}\n",
    "for k in tf.global_variables():\n",
    "    if k not in tf.contrib.framework.get_variables_by_suffix('Momentum'):#Load all parameters except ones of optimization functions\n",
    "            para_dict[k.name[:-2]] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./inqmodel/stage2/inqcom16_97/inqcom1697_92569_4.ckpt\n"
     ]
    }
   ],
   "source": [
    "sess=tf.InteractiveSession()\n",
    "saver = tf.train.Saver(para_dict)\n",
    "#saver.restore(sess,'./inqmodel/stage2/64pinq80/64pinq80ok_93149_7.ckpt')\n",
    "#saver.restore(sess,'./modellog/weightonlypara93.ckpt')\n",
    "saver.restore(sess,'./inqmodel/stage2/inqcom16_97/inqcom1697_92569_4.ckpt')\n",
    "#saver.restore(sess,'./prunemodel/stage2/inc100adj/prune100ar_92969_10ok.ckpt')\n",
    "\n",
    "##################End of Pretrained Parameters Loading############################################### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import config\n",
    "#Nearly all hyperparameters are set in config.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_inq(weights, inq_dict):\n",
    "   \n",
    "    for target in config.all_para:\n",
    "        wl = target\n",
    "        bit = config.inq_para[wl]\n",
    "        # Get target layer's weights\n",
    "        weight_obj = weights[wl]\n",
    "        weight_arr = weight_obj.eval()\n",
    "        \n",
    "        \n",
    "        weight_rest = np.reshape(weight_arr,[-1])\n",
    "        dic_tem = np.reshape(inq_dict[wl],[-1])\n",
    "        idx_rest = np.flip(np.argsort(abs(np.reshape(weight_rest,[-1]))),0) #choose which weights to be INQed\n",
    "        num_prune = int(len(weight_rest)*config.inqpercen_para[wl]*config.inqprune_para[wl])#how many weights to be INQed after pruning\n",
    "     \n",
    "    \n",
    "        #calculate INQ bounds\n",
    "        weight_toINQ = weight_rest[idx_rest[:num_prune]] \n",
    "        n1 = (np.floor(np.log2(max(abs(np.reshape(weight_arr,[-1])))*4/3)))\n",
    "        n2 = n1 +1 - bit/4\n",
    "        print(n1,n2,n1-n2)\n",
    "        upper_bound = 2**(np.floor(np.log2(max(abs(np.reshape(weight_arr,[-1])))*4/3)))\n",
    "        lower_bound = 2**(n1 +1 - bit/4)\n",
    "       \n",
    "    \n",
    "       #INQ\n",
    "        weight_toINQ[abs(weight_toINQ) < lower_bound] = 0\n",
    "        weight_toINQ[weight_toINQ != 0] = 2**(np.floor(np.log2(abs(weight_toINQ[weight_toINQ != 0]*4/3))))*np.sign(weight_toINQ[weight_toINQ != 0])\n",
    "\n",
    "        # Apply pruning\n",
    "        weight_rest[idx_rest[:num_prune]] = weight_toINQ\n",
    "        weight_arr =  np.reshape(weight_rest,np.shape(weight_arr))\n",
    "        dic_tem [idx_rest[:num_prune]] =  np.zeros_like(dic_tem [idx_rest[:num_prune]])\n",
    "        inq_dict[wl] = np.reshape(dic_tem,np.shape(inq_dict[wl]))\n",
    "       # print('left',sum(np.reshape(inq_dict[wl],-1)))\n",
    "\n",
    "        # Store pruned weights as tensorflow objects\n",
    "        sess.run(weight_obj.assign(weight_arr))\n",
    "\n",
    "    return inq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prune_dict = {}\n",
    "for target in config.all_para:\n",
    "    wl =target\n",
    "    weight_obj = para_dict[wl]\n",
    "    prune_dict[wl] = np.ones_like(weight_obj.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0 -4.0 3.0\n",
      "Variable 0.0625 0.176776695297 0.353553390593\n",
      "-2.0 -5.0 3.0\n",
      "Variable_1 0.03125 0.148650889375 0.210224103813\n",
      "-2.0 -5.0 3.0\n",
      "Variable_2 0.03125 0.148650889375 0.210224103813\n",
      "-2.0 -5.0 3.0\n",
      "Variable_3 0.03125 0.148650889375 0.210224103813\n",
      "-2.0 -5.0 3.0\n",
      "Variable_4 0.03125 0.148650889375 0.210224103813\n",
      "-3.0 -6.0 3.0\n",
      "Variable_5 0.015625 0.136313466583 0.114625505401\n",
      "-3.0 -6.0 3.0\n",
      "Variable_6 0.015625 0.136313466583 0.114625505401\n",
      "-3.0 -6.0 3.0\n",
      "Variable_7 0.015625 0.136313466583 0.114625505401\n",
      "-2.0 -5.0 3.0\n",
      "Variable_8 0.03125 0.148650889375 0.210224103813\n",
      "-3.0 -6.0 3.0\n",
      "Variable_9 0.015625 0.136313466583 0.114625505401\n",
      "-3.0 -6.0 3.0\n",
      "Variable_10 0.015625 0.136313466583 0.114625505401\n",
      "-3.0 -6.0 3.0\n",
      "Variable_11 0.015625 0.136313466583 0.114625505401\n",
      "-3.0 -6.0 3.0\n",
      "Variable_12 0.015625 0.136313466583 0.114625505401\n",
      "-2.0 -5.0 3.0\n",
      "Variable_13 0.03125 0.148650889375 0.210224103813\n",
      "-3.0 -6.0 3.0\n",
      "Variable_14 0.015625 0.136313466583 0.114625505401\n",
      "-2.0 -5.0 3.0\n",
      "Variable_15 0.03125 0.148650889375 0.210224103813\n",
      "-2.0 -5.0 3.0\n",
      "Variable_16 0.03125 0.148650889375 0.210224103813\n",
      "-3.0 -6.0 3.0\n",
      "Variable_17 0.015625 0.136313466583 0.114625505401\n",
      "-3.0 -6.0 3.0\n",
      "Variable_18 0.015625 0.136313466583 0.114625505401\n",
      "-3.0 -6.0 3.0\n",
      "Variable_19 0.015625 0.136313466583 0.114625505401\n",
      "-3.0 -6.0 3.0\n",
      "Variable_20 0.015625 0.136313466583 0.114625505401\n",
      "-3.0 -6.0 3.0\n",
      "Variable_21 0.015625 0.136313466583 0.114625505401\n",
      "-3.0 -6.0 3.0\n",
      "Variable_22 0.015625 0.136313466583 0.114625505401\n",
      "-3.0 -6.0 3.0\n",
      "Variable_23 0.015625 0.136313466583 0.114625505401\n",
      "-3.0 -6.0 3.0\n",
      "Variable_24 0.015625 0.136313466583 0.114625505401\n",
      "-3.0 -6.0 3.0\n",
      "Variable_25 0.015625 0.136313466583 0.114625505401\n",
      "-3.0 -6.0 3.0\n",
      "Variable_26 0.015625 0.136313466583 0.114625505401\n",
      "-3.0 -6.0 3.0\n",
      "Variable_27 0.015625 0.136313466583 0.114625505401\n",
      "-3.0 -6.0 3.0\n",
      "Variable_28 0.015625 0.136313466583 0.114625505401\n",
      "-3.0 -6.0 3.0\n",
      "Variable_29 0.015625 0.136313466583 0.114625505401\n",
      "-3.0 -6.0 3.0\n",
      "Variable_30 0.015625 0.136313466583 0.114625505401\n",
      "-3.0 -6.0 3.0\n",
      "Variable_31 0.015625 0.136313466583 0.114625505401\n",
      "-3.0 -6.0 3.0\n",
      "Variable_32 0.015625 0.136313466583 0.114625505401\n",
      "-3.0 -6.0 3.0\n",
      "Variable_33 0.015625 0.136313466583 0.114625505401\n",
      "-3.0 -6.0 3.0\n",
      "Variable_34 0.015625 0.136313466583 0.114625505401\n",
      "-3.0 -6.0 3.0\n",
      "Variable_35 0.015625 0.136313466583 0.114625505401\n",
      "-3.0 -6.0 3.0\n",
      "Variable_36 0.015625 0.136313466583 0.114625505401\n",
      "-3.0 -6.0 3.0\n",
      "Variable_37 0.015625 0.136313466583 0.114625505401\n",
      "-3.0 -6.0 3.0\n",
      "Variable_38 0.015625 0.136313466583 0.114625505401\n",
      "0.0 -3.0 3.0\n",
      "Variable_39 0.125 0.25 0.5\n"
     ]
    }
   ],
   "source": [
    "prune_dict = apply_inq(para_dict, prune_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./inqmodel/stage1/inqcom1697100s.ckpt'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(sess,'./inqmodel/stage1/inqcom1697100s.ckpt')#save INQed parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load pruning mask\n",
    "import pickle\n",
    "f2 = open(\"./prunemodel/stage2/inc100adj/prune100ar.txt\",\"rb\")\n",
    "nz_list = pickle.load(f2)\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in prune_dict.items():\n",
    "    layer = k[0]\n",
    "    prune_dict[k[0]] = np.multiply(prune_dict[k[0]],nz_list[k[0]]) #combine pruning mask and INQ mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save INQ mask\n",
    "import pickle\n",
    "# create dict\n",
    "# save dict\n",
    "f1 = open(\"C:/Users/lhlne/Desktop/project/densenet/inqmodel/stage1/inqcom1697.txt\",\"wb\")\n",
    "pickle.dump(prune_dict, f1)\n",
    "f1.close()\n",
    "# load dict\n",
    "f2 = open(\"C:/Users/lhlne/Desktop/project/densenet/inqmodel/stage1/inqcom1697.txt\",\"rb\")\n",
    "load_list = pickle.load(f2)\n",
    "f2.close()\n",
    "# print \n",
    "print(load_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
